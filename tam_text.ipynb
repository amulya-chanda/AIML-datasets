{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102644fb-7681-47e5-8bb0-baee91e64f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_columns = None\n",
    "# pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed9e018-8fa1-4667-8a39-34fb8e3eaaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company                        object\n",
      "Lists                          object\n",
      "# Employees                   float64\n",
      "Industry                       object\n",
      "Website                        object\n",
      "Company Linkedin Url           object\n",
      "Company City                   object\n",
      "Company State                  object\n",
      "Company Country                object\n",
      "Keywords                       object\n",
      "SEO Description                object\n",
      "Technologies                   object\n",
      "Total Funding                 float64\n",
      "Latest Funding                 object\n",
      "Latest Funding Amount         float64\n",
      "Last Raised At                 object\n",
      "Annual Revenue                float64\n",
      "Number of Retail Locations    float64\n",
      "Short Description              object\n",
      "Founded Year                  float64\n",
      "ICP                            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"/users/amulya/Downloads/text_columns.xlsx\")\n",
    "# print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f918849-0860-410e-9269-732e1ca9b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/amulya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amulya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/amulya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       gainsight customer success product experience ...\n",
      "1                                                  others\n",
      "2       build web mobile apps faster laravel android f...\n",
      "3       runo call management app manage call sim level...\n",
      "4       accelerate ecommerce data analytics journey sa...\n",
      "                              ...                        \n",
      "1712                                    ordering platform\n",
      "1713                                               others\n",
      "1714                                               others\n",
      "1715                                               others\n",
      "1716                                               others\n",
      "Name: SEO Description, Length: 1717, dtype: object\n",
      "Word2Vec<vocab=55, vector_size=100, alpha=0.025>\n",
      "hdgvdbvb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import contractions\n",
    "from num2words import num2words\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "# from bs4 import BeautifulSoup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = pd.read_excel(\"/users/amulya/Downloads/text_columns.xlsx\")\n",
    "a = df.isnull().sum()\n",
    "# print(a)\n",
    "text_columns = ['Keywords','SEO Description','Technologies','Short Description']\n",
    "df['SEO Description'] = df['SEO Description'].fillna('others')\n",
    "df['Keywords'] = df['Keywords'].fillna('others')\n",
    "df['Technologies'] = df['Technologies'].fillna('others')\n",
    "df['Short Description'] = df['Short Description'].fillna('others')\n",
    "\n",
    "\n",
    "# print(df.head())\n",
    "df['SEO Description'] = df['SEO Description'].str.lower()\n",
    "df['SEO Description'] = df['SEO Description'].apply(word_tokenize)\n",
    "punctuation = set(string.punctuation)\n",
    "df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [token for token in tokens if token not in punctuation])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [num2words(token) if token.isdigit() else token for token in tokens])\n",
    "df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [contractions.fix(token) for token in tokens])\n",
    "df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [token for token in tokens if token.isalnum()])\n",
    "df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [token.strip() for token in tokens])\n",
    "df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [re.sub(r'http\\S+', '', token) for token in tokens])\n",
    "df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [re.sub(r'[\\[\\](){}]', '', token) for token in tokens])\n",
    "\n",
    "# df['SEO Description'] = df['SEO Description'].apply(lambda text: BeautifulSoup(text, \"html.parser\").get_text())\n",
    "spell = SpellChecker()\n",
    "for tokens in df['SEO Description']:\n",
    "    for token in tokens:\n",
    "        if token is None:\n",
    "           pass\n",
    "        else:\n",
    "            corrected_token = spell.correction(token)\n",
    "           \n",
    "# Join tokens back to strings\n",
    "df['SEO Description'] = df['SEO Description'].apply(' '.join)\n",
    "\n",
    "print(df['SEO Description'])\n",
    "w2v_model = Word2Vec(sentences=df['SEO Description'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "w2v_model.train(df['SEO Description'], total_examples=len(df['SEO Description']), epochs=10)\n",
    "word_vectors = w2v_model.wv\n",
    "print(w2v_model)\n",
    "print('hdgvdbvb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3520f118-c2b8-44d8-b233-fd1a2fc21394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ikdjshbn\n",
      "ikdjshbn\n",
      "ikdjshbn\n"
     ]
    },
    {
     "ename": "InvalidOperation",
     "evalue": "[<class 'decimal.ConversionSyntax'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidOperation\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to each text column\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m text_columns:\n\u001b[0;32m---> 43\u001b[0m     df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     tokenized_text \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     45\u001b[0m     word2vec_model \u001b[38;5;241m=\u001b[39m train_word2vec_model(tokenized_text)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/pandas/core/series.py:4897\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4780\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m----> 9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [num2words(token) \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;28;01melse\u001b[39;00m token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [contractions\u001b[38;5;241m.\u001b[39mfix(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalnum()]\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m----> 9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mnum2words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;28;01melse\u001b[39;00m token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [contractions\u001b[38;5;241m.\u001b[39mfix(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalnum()]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/num2words/__init__.py:95\u001b[0m, in \u001b[0;36mnum2words\u001b[0;34m(number, ordinal, lang, to, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m converter \u001b[38;5;241m=\u001b[39m CONVERTER_CLASSES[lang]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(number, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_number\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# backwards compatible\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ordinal:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/num2words/base.py:101\u001b[0m, in \u001b[0;36mNum2Word_Base.str_to_number\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstr_to_number\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDecimal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInvalidOperation\u001b[0m: [<class 'decimal.ConversionSyntax'>]"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [num2words(token) if token.isdigit() else token for token in tokens]\n",
    "    tokens = [contractions.fix(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    tokens = [re.sub(r'http\\S+', '', token) for token in tokens]\n",
    "    spell = SpellChecker()\n",
    "    corrected_tokens = []\n",
    "    for token in tokens:\n",
    "        corrected_token = spell.correction(token)\n",
    "        corrected_tokens.append(corrected_token)\n",
    "\n",
    "    return corrected_tokens\n",
    "def train_word2vec_model(tokenized_text):\n",
    "    word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    word2vec_model.train(tokenized_text, total_examples=len(tokenized_text), epochs=10)\n",
    "    return word2vec_model\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_excel(\"/users/amulya/Downloads/text_columns.xlsx\")\n",
    "\n",
    "# Define the text columns\n",
    "text_columns = ['Keywords', 'SEO Description', 'Technologies', 'Short Description']\n",
    "df['Keywords'] = df['Keywords'].astype(str)\n",
    "df['SEO Description'] = df['SEO Description'].astype(str)\n",
    "df['Technologies'] = df['Technologies'].astype(str)\n",
    "df['Short Description'] = df['Short Description'].astype(str)\n",
    "df['SEO Description'] = df['SEO Description'].fillna('others')\n",
    "df['Keywords'] = df['Keywords'].fillna('others')\n",
    "df['Technologies'] = df['Technologies'].fillna('others')\n",
    "df['Short Description'] = df['Short Description'].fillna('others')\n",
    "word2vec_models = {}\n",
    "\n",
    "# Apply preprocessing to each text column\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].apply(preprocess_text)\n",
    "    tokenized_text = df[col].tolist()\n",
    "    word2vec_model = train_word2vec_model(tokenized_text)\n",
    "    word2vec_models[col] = word2vec_model\n",
    "    # print(word2vec_model.wv.most_similar('example'))\n",
    "    print('ikdjshbn')\n",
    "print(df['Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d466e9-6c91-47cc-ac85-de42f987e277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e68235-05b8-4cc6-9f9a-92d1a365583d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca65d87-1375-4d60-9779-1ce2c53df216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850f19e-9053-493b-929f-a0a523ff2c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
