{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1702cb7-9485-422d-8510-6084cebd0bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/amulya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amulya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/amulya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m            \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m             corrected_token \u001b[38;5;241m=\u001b[39m \u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Join tokens back to strings\u001b[39;00m\n\u001b[1;32m    119\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTechnologies\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTechnologies\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/spellchecker/spellchecker.py:158\u001b[0m, in \u001b[0;36mSpellChecker.correction\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The most probable correct spelling for the word\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    word (str): The word to correct\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    str: The most likely candidate or None if no correction is present\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m word \u001b[38;5;241m=\u001b[39m ensure_unicode(word)\n\u001b[0;32m--> 158\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/spellchecker/spellchecker.py:185\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/spellchecker/spellchecker.py:252\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medit_distance_1(e1))]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/spellchecker/spellchecker.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/spellchecker/spellchecker.py:199\u001b[0m, in \u001b[0;36mSpellChecker.known\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    197\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    198\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/amulya_base/lib/python3.9/site-packages/spellchecker/spellchecker.py:199\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    197\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    198\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_word_frequency\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdictionary\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import contractions\n",
    "from num2words import num2words\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "# from bs4 import BeautifulSoup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = pd.read_excel(\"/users/amulya/Downloads/text_columns.xlsx\")\n",
    "a = df.isnull().sum()\n",
    "# print(a)\n",
    "text_columns = ['Keywords','SEO Description','Technologies','Short Description']\n",
    "df['SEO Description'] = df['SEO Description'].fillna('others')\n",
    "df['Keywords'] = df['Keywords'].fillna('others')\n",
    "df['Technologies'] = df['Technologies'].fillna('others')\n",
    "df['Short Description'] = df['Short Description'].fillna('others')\n",
    "\n",
    "\n",
    "# # print(df.head())\n",
    "# df['SEO Description'] = df['SEO Description'].str.lower()\n",
    "# df['SEO Description'] = df['SEO Description'].apply(word_tokenize)\n",
    "# punctuation = set(string.punctuation)\n",
    "# df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [token for token in tokens if token not in punctuation])\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "# df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [num2words(token) if token.isdigit() else token for token in tokens])\n",
    "# df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [contractions.fix(token) for token in tokens])\n",
    "# df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [token for token in tokens if token.isalnum()])\n",
    "# df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [token.strip() for token in tokens])\n",
    "# df['SEO Description'] = df['SEO Description'].apply(lambda tokens: [re.sub(r'http\\S+', '', token) for token in tokens])\n",
    "# # df['SEO Description'] = df['SEO Description'].apply(lambda text: BeautifulSoup(text, \"html.parser\").get_text())\n",
    "# spell = SpellChecker()\n",
    "# for tokens in df['SEO Description']:\n",
    "#     for token in tokens:\n",
    "#         if token is None:\n",
    "#            pass\n",
    "#         else:\n",
    "#             corrected_token = spell.correction(token)\n",
    "           \n",
    "# # Join tokens back to strings\n",
    "# df['SEO Description'] = df['SEO Description'].apply(' '.join)\n",
    "\n",
    "# print(df['SEO Description'])\n",
    "# w2v_model = Word2Vec(sentences=df['SEO Description'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "# w2v_model.train(df['SEO Description'], total_examples=len(df['SEO Description']), epochs=10)\n",
    "# word_vectors = w2v_model.wv\n",
    "# print(w2v_model)\n",
    "# print(df['SEO Description'])\n",
    "\n",
    "# df['Keywords'] = df['Keywords'].str.lower()\n",
    "# df['Keywords'] = df['Keywords'].apply(word_tokenize)\n",
    "# punctuation = set(string.punctuation)\n",
    "# df['Keywords'] = df['Keywords'].apply(lambda tokens: [token for token in tokens if token not in punctuation])\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['Keywords'] = df['Keywords'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# df['Keywords'] = df['Keywords'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "# df['Keywords'] = df['Keywords'].apply(lambda tokens: [num2words(token) if token.isdigit() else token for token in tokens])\n",
    "# df['Keywords'] = df['Keywords'].apply(lambda tokens: [contractions.fix(token) for token in tokens])\n",
    "# df['Keywords'] = df['Keywords'].apply(lambda tokens: [token for token in tokens if token.isalnum()])\n",
    "# df['Keywords'] = df['Keywords'].apply(lambda tokens: [token.strip() for token in tokens])\n",
    "# df['Keywords'] = df['Keywords'].apply(lambda tokens: [re.sub(r'http\\S+', '', token) for token in tokens])\n",
    "# # df['Keywords'] = df['Keywords'].apply(lambda text: BeautifulSoup(text, \"html.parser\").get_text())\n",
    "# spell = SpellChecker()\n",
    "# for tokens in df['Keywords']:\n",
    "#     for token in tokens:\n",
    "#         if token is None:\n",
    "#            pass\n",
    "#         else:\n",
    "#             corrected_token = spell.correction(token)\n",
    "           \n",
    "# # Join tokens back to strings\n",
    "# df['Keywords'] = df['Keywords'].apply(' '.join)\n",
    "\n",
    "# print(df['Keywords'])\n",
    "# w2v_model = Word2Vec(sentences=df['Keywords'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "# w2v_model.train(df['Keywords'], total_examples=len(df['Keywords']), epochs=10)\n",
    "# word_vectors = w2v_model.wv\n",
    "# print(w2v_model)\n",
    "# print(df['Keywords'])\n",
    "\n",
    "df['Technologies'] = df['Technologies'].str.lower()\n",
    "df['Technologies'] = df['Technologies'].apply(word_tokenize)\n",
    "punctuation = set(string.punctuation)\n",
    "df['Technologies'] = df['Technologies'].apply(lambda tokens: [token for token in tokens if token not in punctuation])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Technologies'] = df['Technologies'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Technologies'] = df['Technologies'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "df['Technologies'] = df['Technologies'].apply(lambda tokens: [num2words(token) if token.isdigit() else token for token in tokens])\n",
    "df['Technologies'] = df['Technologies'].apply(lambda tokens: [contractions.fix(token) for token in tokens])\n",
    "df['Technologies'] = df['Technologies'].apply(lambda tokens: [token for token in tokens if token.isalnum()])\n",
    "df['Technologies'] = df['Technologies'].apply(lambda tokens: [token.strip() for token in tokens])\n",
    "df['Technologies'] = df['Technologies'].apply(lambda tokens: [re.sub(r'http\\S+', '', token) for token in tokens])\n",
    "# df['Technologies'] = df['Technologies'].apply(lambda text: BeautifulSoup(text, \"html.parser\").get_text())\n",
    "spell = SpellChecker()\n",
    "for tokens in df['Technologies']:\n",
    "    for token in tokens:\n",
    "        if token is None:\n",
    "           pass\n",
    "        else:\n",
    "            corrected_token = spell.correction(token)\n",
    "           \n",
    "# Join tokens back to strings\n",
    "df['Technologies'] = df['Technologies'].apply(' '.join)\n",
    "\n",
    "print(df['Technologies'])\n",
    "w2v_model = Word2Vec(sentences=df['Technologies'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "w2v_model.train(df['Technologies'], total_examples=len(df['Technologies']), epochs=10)\n",
    "word_vectors = w2v_model.wv\n",
    "print(w2v_model)\n",
    "print(df['Technologies'])\n",
    "\n",
    "# df['Short Description'] = df['Short Description'].str.lower()\n",
    "# df['Short Description'] = df['Short Description'].apply(word_tokenize)\n",
    "# punctuation = set(string.punctuation)\n",
    "# df['Short Description'] = df['Short Description'].apply(lambda tokens: [token for token in tokens if token not in punctuation])\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['Short Description'] = df['Short Description'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# df['Short Description'] = df['Short Description'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "# df['Short Description'] = df['Short Description'].apply(lambda tokens: [num2words(token) if token.isdigit() else token for token in tokens])\n",
    "# df['Short Description'] = df['Short Description'].apply(lambda tokens: [contractions.fix(token) for token in tokens])\n",
    "# df['Short Description'] = df['Short Description'].apply(lambda tokens: [token for token in tokens if token.isalnum()])\n",
    "# df['Short Description'] = df['Short Description'].apply(lambda tokens: [token.strip() for token in tokens])\n",
    "# df['Short Description'] = df['Short Description'].apply(lambda tokens: [re.sub(r'http\\S+', '', token) for token in tokens])\n",
    "# # df['Short Description'] = df['Short Description'].apply(lambda text: BeautifulSoup(text, \"html.parser\").get_text())\n",
    "# spell = SpellChecker()\n",
    "# for tokens in df['Short Description']:\n",
    "#     for token in tokens:\n",
    "#         if token is None:\n",
    "#            pass\n",
    "#         else:\n",
    "#             corrected_token = spell.correction(token)\n",
    "           \n",
    "# # Join tokens back to strings\n",
    "# df['Short Description'] = df['Short Description'].apply(' '.join)\n",
    "\n",
    "# print(df['Short Description'])\n",
    "# w2v_model = Word2Vec(sentences=df['Short Description'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "# w2v_model.train(df['Short Description'], total_examples=len(df['Short Description']), epochs=10)\n",
    "# word_vectors = w2v_model.wv\n",
    "# print(w2v_model)\n",
    "# print(df['Short Description'])\n",
    "\n",
    "\n",
    "words = df['Technologies'].str.split()\n",
    "\n",
    "# Get unique words\n",
    "unique_words = set(word for sublist in words for word in sublist)\n",
    "\n",
    "# Create a DataFrame to store the binary representation of words\n",
    "binary_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over unique words and create binary columns\n",
    "for word in unique_words:\n",
    "    binary_df[word] = df['Technologies'].apply(lambda x: 1 if word in x.split() else 0)\n",
    "\n",
    "# Concatenate the binary DataFrame with the original DataFrame\n",
    "df_binary = pd.concat([df, binary_df], axis=1)\n",
    "\n",
    "# Drop the original 'SEO Description' column\n",
    "# df_binary.drop(columns=['SEO Description'], inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb420a-8390-4fca-a43e-3e431f14fa55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
